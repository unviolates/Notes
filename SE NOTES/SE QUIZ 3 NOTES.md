Comprehensive Software Engineering Study GuideCourse Code: CSC291Based on: Software Engineering: A Practitioner's Approach (Roger S. Pressman)Focus: Post-Midterm Topics (Sequence Diagrams, Quality Concepts, Reviews, SQA)Table of ContentsModule 1: Interaction Modeling with Sequence Diagrams1.1 Introduction to Behavioral Modeling1.2 The Anatomy of a Sequence Diagram1.3 Message Types and Notation1.4 Control Structures and Combined Fragments1.5 The Process of Creating a Sequence Diagram1.6 Practical Example: SafeHome Security System1.7 Best Practices for Sequence DiagrammingModule 2: Software Quality Concepts2.1 Defining Software Quality2.2 Garvin’s Quality Dimensions2.3 McCall’s Quality Factors2.4 ISO 9126 Quality Factors2.5 Targeted Quality Factors2.6 The Software Quality Dilemma2.7 The Cost of Quality2.8 Achieving Software QualityModule 3: Review Techniques3.1 The Strategic Importance of Reviews3.2 Cost Impact of Software Defects3.3 Defect Amplification and Removal3.4 Review Metrics and Their Use3.5 The Review Formality Spectrum3.6 Informal Reviews3.7 Formal Technical Reviews (FTR)3.8 Review Reporting and Record Keeping3.9 Guidelines for Effective ReviewsModule 4: Software Quality Assurance (SQA)4.1 Background and Definition4.2 Elements of Software Quality Assurance4.3 SQA Tasks4.4 SQA Goals, Attributes, and Metrics4.5 Statistical Software Quality Assurance4.6 Six Sigma for Software Engineering4.7 Software Reliability4.8 Software Safety4.9 ISO 9000 Quality Standards4.10 The SQA PlanModule 1: Interaction Modeling with Sequence Diagrams1.1 Introduction to Behavioral ModelingIn the context of software engineering, specifically within the Unified Modeling Language (UML), Sequence Diagrams are a form of interaction diagram. While structural diagrams (like Class Diagrams) depict the static architecture of a system, Sequence Diagrams model the dynamic behavior. They are essential for understanding how objects within the system collaborate over time to achieve a specific function or Use Case.The primary purpose of a sequence diagram is to define event sequences that result in some desired behavior. They are particularly useful during the design phase to verify that the classes defined in the structural model have sufficient methods to perform the required tasks.1.2 The Anatomy of a Sequence DiagramA sequence diagram is structured as a two-dimensional chart. The horizontal axis represents the objects (or participating entities) involved in the interaction, while the vertical axis represents time, proceeding down the page.1.2.1 Actors and ObjectsAt the top of the diagram, you will find the participants. These can be:Actors: External entities (users, external systems) that initiate or participate in the process. Represented by a stick figure.Objects: Instances of classes within the system. Represented by a rectangle containing the name instanceName : ClassName. If the instance name is omitted (:ClassName), it represents an anonymous instance of that class.1.2.2 LifelinesExtending downward from each object is a dashed vertical line known as the Lifeline. This line represents the existence of the object over the duration of the interaction.If an object is created during the interaction, its lifeline starts at the point of creation.If an object is destroyed during the interaction, its lifeline ends with a large "X" at the point of destruction.1.2.3 Activation (Focus of Control)Superimposed on the lifeline is a thin, vertical rectangle called the Activation Bar or Focus of Control. This rectangle indicates the period during which the object is actively performing an action, either by executing its own code or by waiting for a response from another object (in the case of synchronous calls).The length of the bar corresponds to the duration of the activity.Stacked activation bars indicate nested calls (recursion or internal self-calls).1.3 Message Types and NotationCommunication between objects is depicted as horizontal arrows, or messages, connecting their lifelines. The type of arrow and line style conveys the semantic meaning of the message.1.3.1 Synchronous MessageNotation: Solid line with a filled (solid) arrowhead.Meaning: The sender sends a message and halts its own execution (blocks) until it receives a reply from the receiver. This is typical of a standard function or method call in most programming languages (e.g., objectA.methodB()).1.3.2 Asynchronous MessageNotation: Solid line with an open (stick) arrowhead.Meaning: The sender transmits the message and continues its own execution immediately without waiting for a response. This is common in multi-threaded environments, event-driven programming, or message queue systems.1.3.3 Return MessageNotation: Dashed line with an open arrowhead pointing back to the sender.Meaning: Represents the return of control and/or data from the receiver back to the sender after a synchronous call has completed. In many modern diagrams, the return arrow is implicit and omitted to reduce clutter, unless a specific return value needs to be emphasized.1.3.4 Self MessageNotation: An arrow that starts and ends on the same lifeline.Meaning: The object calls a method on itself. This often results in a nested activation bar.1.3.5 Create MessageNotation: A dashed line with an open arrowhead pointing to the "head" of an object rectangle.Meaning: The message instantiates a new object. The lifeline of the created object begins at the point where this message arrives.1.3.6 Destroy MessageNotation: A solid line with a solid arrowhead terminating in a large "X" on the target lifeline.Meaning: The object is removed from memory or ceases to exist.1.4 Control Structures and Combined FragmentsSimple linear sequences are rarely sufficient to model complex software logic. To handle conditional logic, loops, and parallel processing, UML 2.0 introduced Combined Fragments. These are represented by a box (frame) enclosing a portion of the interaction.1.4.1 Alternative (Alt)Logic: Equivalent to an if... else statement.Representation: A frame labeled alt. The frame is divided into horizontal regions by a dashed line. Each region has a "guard condition" (e.g., [balance > 0]). Only the region where the guard condition is true is executed.1.4.2 Option (Opt)Logic: Equivalent to a simple if statement (without an else).Representation: A frame labeled opt with a guard condition. The interaction inside occurs only if the condition is true; otherwise, the sequence skips over it.1.4.3 Loop (Loop)Logic: Equivalent to a for or while loop.Representation: A frame labeled loop. It may include a guard condition (e.g., [i < 10]) or a specific number of iterations (e.g., (1, n)). The sequence inside represents the body of the loop.1.4.4 Parallel (Par)Logic: Concurrent processing.Representation: A frame labeled par. The frame is divided into regions. The interactions in the different regions occur simultaneously (or interleaved) in time.1.5 The Process of Creating a Sequence DiagramTo create an effective sequence diagram, a software engineer typically follows these steps:Identify the Scope: Determine the specific Use Case or scenario you are modeling. Do not try to model the entire system in one diagram.Identify the Participants: List the actors and the primary objects/classes involved in this scenario.Layout the Lifelines: Place the primary actor on the far left and the other objects to the right. Order them to minimize line crossing if possible.Trace the Message Flow: Step through the logic of the Use Case:"The user clicks login." (Message from Actor to UI)"The UI validates the format." (Self message)"The UI requests authentication." (Message from UI to SecurityManager)"The SecurityManager checks the database." (Message from SecurityManager to UserDatabase)Add Control Structures: Identify where choices or loops occur and wrap those sections in alt or loop fragments.Refine and Validate: Check against the Class Diagram to ensure the methods called actually exist (or add them to the Class Diagram if they are new discoveries).Module 2: Software Quality ConceptsReference: Pressman Chapter 142.1 Defining Software QualityQuality is a somewhat elusive concept, often defined subjectively ("I know it when I see it"). However, in software engineering, vague definitions are dangerous. Pressman defines software quality as:"An effective software process applied in a manner that creates a useful product that provides measurable value for those who produce it and those who use it."This definition breaks down into three critical components:Effective Software Process: This establishes the infrastructure. It includes management checks and balances to avoid chaos. It implies that good quality is not an accident; it is the result of deliberate planning and execution.Useful Product: The software must deliver the content, functions, and features the end-user desires. It must also satisfy implicit requirements (like ease of use). If a product is bug-free but doesn't do what the user needs, it is not "high quality."Measurable Value:For the Producer: High quality results in less maintenance effort, fewer bug fixes, and reduced support costs.For the User: The application expedites business processes and provides crucial information reliably.2.2 Garvin’s Quality DimensionsDavid Garvin suggested a multidimensional view of quality. While originally for manufacturing, these translate directly to software:Performance Quality: Does the software deliver the specified content, functions, and features? Does it do what it claims to do?Feature Quality: Does the software provide features that "surprise and delight" the user? These are often the differentiators that go beyond basic functional requirements.Reliability: Does the software deliver capability without failure? Is it available when needed?Conformance: Does the software conform to local and external standards (e.g., coding standards, interface design rules)?Durability: Can the software be maintained or corrected without unintended side effects? (Often linked to the concept of "regression").Serviceability: Can the software be fixed in an acceptably short time period? Can support staff easily debug it?Aesthetics: A subjective view. Does the interface have elegance and flow? Is it pleasing to look at?Perception: The user’s bias. If a vendor has a bad reputation, the user may perceive the software as low quality even if it is not.2.3 McCall’s Quality FactorsMcCall, Richards, and Walters categorized quality into three broad domains, known as the "Quality Triangle." These factors provide a basis for indirect measurement of quality.Getty Images2.3.1 Product OperationFocuses on the software in action.Correctness: Does it satisfy the specification?Reliability: Does it perform with required precision?Efficiency: Does it use resources (memory, CPU) optimally?Integrity: Is access controlled (security)?Usability: Is it easy to learn and operate?2.3.2 Product RevisionFocuses on the ability to change the software.Maintainability: Effort required to locate and fix an error.Flexibility: Effort required to modify an operational program.Testability: Effort required to test the program.2.3.3 Product TransitionFocuses on moving the software to new environments.Portability: Effort to transfer the program to different hardware/OS.Reusability: Extent to which parts of the program can be reused in other apps.Interoperability: Effort required to couple the system to others.2.4 ISO 9126 Quality FactorsThe ISO 9126 standard attempts to standardize these definitions into six key attributes:Functionality: Suitability, accuracy, interoperability, security.Reliability: Maturity, fault tolerance, recoverability.Usability: Understandability, learnability, operability.Efficiency: Time behavior (speed), resource behavior (memory).Maintainability: Analyzability, changeability, stability, testability.Portability: Adaptability, installability, conformance.2.5 The Software Quality DilemmaThe "Dilemma" refers to the tension between "Total Quality" and "Time/Cost."Bertrand Meyer described this: "If you spend infinite time... to build the absolutely perfect piece of software... you'll be out of business... You missed the market window."2.5.1 "Good Enough" SoftwareThis philosophy argues that it is acceptable to release software with known bugs if it delivers high-quality core functions and reaches the market on time.The Argument For: Market dominance (locking in customers with v1.0).The Argument Against: Reputation damage. In safety-critical domains (medical, avionics), "good enough" is negligent and potentially criminal.2.5.2 The Cost of QualityQuality has a cost, but lack of quality costs more.Prevention Costs: Money spent to stop errors from occurring (Training, Planning, Reviews). This is the cheapest way to manage quality.Appraisal Costs: Money spent to find errors (Testing, Inspections).Failure Costs:Internal Failure: Fixing errors found before shipping (Rework).External Failure: Fixing errors found after shipping (Support, Warranty, Lawsuits, Reputation Loss). This is the most expensive cost.Key Insight: One dollar spent on prevention can save hundreds of dollars in external failure costs.2.6 Achieving Software QualityQuality is not accidental. It is achieved through:Software Engineering Methods: Using proper analysis and design techniques (e.g., UML, Design Patterns).Project Management: Proper estimation and risk planning.Quality Control: Actions to find defects (Testing, Reviews).Quality Assurance: The management infrastructure (Audits, Reporting).Module 3: Review TechniquesReference: Pressman Chapter 153.1 The Strategic Importance of ReviewsSoftware reviews are a "filter" for the software process. They are applied at various points during software engineering to uncover errors and defects that can then be removed.Freedman and Weinberg's Philosophy: Technical work needs reviewing for the same reason pencils need erasers: "To err is human."The Goal: To purify the work products (requirements, design, code) before they move to the next stage.3.2 Defect Amplification and RemovalThe Defect Amplification Model illustrates why reviews are critical.Imagine a design step introduces 10 errors.Scenario A (No Reviews): These 10 errors pass to coding. During coding, the developer might misinterpret the erroneous design, creating new errors based on the old ones. The 10 design errors might be "amplified" to 20 or 30 coding errors.Scenario B (With Reviews): A design review catches 8 of the 10 errors. Only 2 pass to coding. The amplification effect is drastically reduced.Statistical Evidence: Industry studies indicate that design activities introduce between 50-65% of all errors. Formal reviews have been shown to be up to 75% effective in uncovering these design flaws.3.3 Review Metrics and Their UseTo measure the effectiveness of reviews, we collect specific metrics:Preparation Effort ($E_p$): Effort (in person-hours) required to review a work product before the meeting.Assessment Effort ($E_a$): Effort expended during the review meeting.Rework Effort ($E_r$): Effort dedicated to correcting the errors found.Work Product Size (WPS): E.g., number of pages, UML models, or lines of code.Errors Found ($Err_{tot}$): Total minor and major errors.Derived Metrics:Error Density: $Err_{tot} / WPS$. (e.g., 1.2 errors per page).Cost Effectiveness: By comparing the cost of finding an error in review (approx. several person-hours) vs. the cost of finding it in testing (approx. 15-40x higher), we can calculate the ROI of the review process.3.4 The Review Formality SpectrumNot all reviews are the same. They range from casual to rigorous. The "Reference Model" for review formality includes four characteristics:Roles: Are roles explicitly defined? (e.g., Recorder, Leader).Planning: Is there advance preparation?Structure: Is there a defined agenda?Correction: Is there a formal verification of the fixes?3.4.1 Informal ReviewsDesk Check: A simple review where a colleague looks over your work. No agenda, no formal log.Pair Programming: A continuous, real-time informal review where two developers work on the same code at one workstation.3.4.2 Formal Technical Reviews (FTR)An FTR is a software quality control activity performed by software engineers.Objectives:Uncover errors in logic, function, or implementation.Verify software meets requirements.Ensure software complies with standards.Achieve uniformity in development.3.5 The FTR Process3.5.1 The Meeting ConstraintsParticipants: Between 3 and 5 people.Preparation: Advance preparation should take no more than 2 hours per person.Duration: The meeting should be less than 2 hours.Scope: Focus on a specific, small part of the software (e.g., one component or class).3.5.2 RolesThe efficacy of a review depends on specific roles being fulfilled effectively.Producer: This is the individual who has developed the work product. Their role is to inform the project leader that the work product is complete and that a review is required. During the meeting, they present or "walk through" the product, explaining the material while reviewers raise issues.Review Leader: This individual is responsible for evaluating the product for readiness, generating copies of materials, and distributing them to reviewers. They also schedule the meeting and prepare the agenda. During the review, the leader ensures the meeting stays on track and halts the review if it gets out of control.Recorder: This is a reviewer who takes on the specific role of recording (in writing) all important issues raised during the review. They document every error or problem discovered so that a formal "issues list" can be created.Reviewers: These are individuals (typically 2 or 3) who are expected to spend between one and two hours in advance preparation. They review the product, make notes, and become familiar with the work so they can raise substantive issues during the meeting.3.5.3 The DecisionAt the end of the review, the team must make one of three decisions:Accept: The product is accepted without further modification.Reject: The product has severe errors; it must be fixed and another review must be performed.Accept Provisionally: Minor errors exist. The producer must fix them, but no second review is required (the leader verifies the fix).3.6 Guidelines for Effective Reviews (The "Dos and Don'ts")Review the Product, Not the Producer: This is the most critical guideline. Egos are sensitive. Point out errors gently. The goal is to improve the software, not to judge the developer's capability.Set an Agenda and Maintain It: Prevent "drift." Do not let the meeting wander into other topics.Limit Debate: If an issue is raised, record it and move on. Do not argue about why it is an error or how to fix it during the meeting.Enunciate Problem Areas, But Don't Solve Them: A review is for finding errors, not fixing them. Problem-solving slows down the meeting.Take Written Notes: The Recorder's job is vital. If it isn't written down, it won't get fixed.Limit Participants: "Two heads are better than one, but 14 are not better than 4." Large groups stifle communication.Insist on Preparation: If reviewers haven't read the document beforehand, the meeting is a waste of time. Cancel it.Develop a Checklist: Use a cheat sheet to ensure common errors (e.g., "Are all variables initialized?") are checked.3.7 Sample-Driven ReviewsIn the real world, resources are limited. We cannot formally review every line of code.Strategy: Inspect a fraction ($a_i$) of each work product.Extrapolation: Record faults found ($f$). Estimate total faults ($1/a_i \times f$).Targeting: Focus full FTR resources on the work products with the highest estimated fault density.Module 4: Software Quality Assurance (SQA)Reference: Pressman Chapter 164.1 Background and DefinitionWhile Quality Control (QC) focuses on finding bugs (Testing, Reviews), Software Quality Assurance (SQA) is the management structure that ensures QC is happening.QC: "Are we building the product right?" (Operational)SQA: "Are we doing the right things to ensure quality?" (Managerial)The SQA group serves as the customer's in-house representative. They look at the software from the customer's point of view.4.2 Elements of SQASQA encompasses a broad range of activities:Standards: Ensuring the team follows IEEE or ISO standards.Reviews and Audits: Ensuring reviews are actually happening and are effective.Testing: Ensuring testing is properly planned (not just performed).Error/Defect Collection: Collecting data to improve the process.Change Management: Ensuring changes are controlled (not chaotic).Education: Training the team in better methods.Vendor Management: Ensuring external software libraries are high quality.Security Management: Ensuring data protection is part of the process.Safety: Assessing impact of failure in critical systems.Risk Management: Ensuring contingency plans exist.4.3 SQA TasksThe SQA group typically performs the following actions:Prepares an SQA Plan: A roadmap for quality activities.Participates in Process Description: Reviews the team's process choice.Reviews Activities: Verifies compliance with the defined process.Audits Work Products: randomly checks documents/code to ensure they match standards.Ensures Deviation Handling: Tracks when the team deviates from the plan and ensures it is documented.Records and Reports: Reports noncompliance to senior management.4.4 SQA Goals, Attributes, and MetricsSQA aims to achieve quality in specific areas, measurable by metrics:Requirements Quality:Attribute: Ambiguity.Metric: Number of ambiguous modifiers (e.g., "fast", "easy") in the spec.Design Quality:Attribute: Complexity.Metric: Coupling and Cohesion measures.Code Quality:Attribute: Maintainability.Metric: Cyclomatic Complexity, Lines of Code.QC Effectiveness:Attribute: Resource Allocation.Metric: Percent of effort spent on reviews vs. testing.4.5 Statistical Software Quality AssuranceThis approach uses math to improve the process.Collect data on all defects.Trace each defect to its root cause (e.g., "Logic Error", "Spec Ambiguity").Pareto Principle (80/20 Rule): 80% of defects arise from 20% of the causes.Action: Isolate the "Vital Few" causes and fix the process to eliminate them.Example: If 50% of errors are "Interface Mismatches," the SQA team might mandate a formal Interface Definition Language (IDL) review step to eliminate that cause.4.6 Six Sigma for SoftwareSix Sigma is a rigorous statistical methodology.Goal: 3.4 defects per million opportunities (near perfection).DMAIC: Used for improving an existing process.DefineMeasureAnalyzeImproveControlDMADV: Used for designing a new process.DefineMeasureAnalyzeDesignVerify4.7 Software ReliabilityReliability is "The probability of failure-free operation... in a specified environment for a specified time."4.7.1 Measures of ReliabilityMTBF (Mean Time Between Failure): $MTBF = MTTF + MTTR$.MTTF: Mean Time To Failure (uptime).MTTR: Mean Time To Repair (downtime).Availability: $Availability = (MTTF / (MTTF + MTTR)) \times 100\%$.Insight: You can improve availability by increasing reliability (MTTF) OR by improving maintainability (decreasing MTTR).4.8 Software SafetySafety is distinct from Reliability.Reliability: Probability of failure.Safety: Probability that a failure causes a catastrophic mishap (death, injury, massive economic loss).Technique: SQA uses Fault Tree Analysis to trace potential hazards back to the software faults that could cause them. The goal is to design "Fail-Safe" systems.4.9 The SQA PlanThe SQA Plan is a document that serves as the "Quality Contract" for the project. It outlines:Purpose and Scope.Documentation standards.The Standards and Practices to be applied.The Reviews and Audits to be conducted.Problem Reporting and Corrective Action procedures.Tools and Methods to be used.
